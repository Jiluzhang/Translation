# scTranslator: https://github.com/TencentAILabHealthcare/scTranslator
# performer: https://github.com/lucidrains/performer-pytorch
# transformer test: https://github.com/rogershijin/GANOLI

conda create -n scTranslator python=3.8.13
pip install scipy==1.9.1 numpy==1.21.5 pandas==1.5.1 scikit-learn==1.1.2 scanpy==1.9.1  # numba 0.58.1 requires numpy<1.27, >=1.22 (1.24.4)
pip install matplotlib==3.6.3  # import scanpy bug: TypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases
pip install einops local_attention
conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch  # debug: RuntimeError: CUDA error: no kernel image is available for execution on the device
wget -c https://codeload.github.com/TencentAILabHealthcare/scTranslator/zip/refs/heads/main

scp -P 6122 GSM2685244_protein_3_PBMCs_matrix_mapped.h5ad GSM2685244_mRNA_3_PBMCs_matrix_mapped.h5ad jiluzhang@10.11.41.108:/fs/home/jiluzhang/scTranslator/dataset/test/dataset2
scp -P 6122 stage2_single-cell_scTranslator.pt jiluzhang@10.11.41.108:/fs/home/jiluzhang/scTranslator/checkpoint

############################################################################################################
python code/stage3_inference_without_finetune.py \
--pretrain_checkpoint='checkpoint/stage2_single-cell_scTranslator.pt' \
--RNA_path='dataset/test/dataset2/GSM2685244_mRNA_3_PBMCs_matrix_mapped.h5ad' \
--Pro_path='dataset/test/dataset2/GSM2685244_protein_3_PBMCs_matrix_mapped.h5ad'

#Total number of origin RNA genes:  21005
#Total number of origin proteins:  44
#Total number of origin cells:  4330
## of NAN in X 0
## of NAN in X 0
#load data ended
#----------------------------------------
#single cell 20000 RNA To 1000 Protein on dataset/new_data-without_fine-tune
#Overall performance in repeat_1 costTime: 168.5123s
#Test Set: AVG mse 0.0408, AVG ccc 0.3924

#real	2m51.998s
#user	87m39.110s
#sys	1m50.236s

# output files
# args1.txt & performance_log.csv
############################################################################################################


## distributed training: https://zhuanlan.zhihu.com/p/76638962
python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node 2 --master_addr '10.11.41.108' --master_port 6123 \
code/stage3_fine-tune.py  --epoch=1 --frac_finetune_test=0.1 --fix_set \
--pretrain_checkpoint='checkpoint/stage2_single-cell_scTranslator.pt' \
--RNA_path='dataset/test/dataset2/GSM2685244_mRNA_3_PBMCs_matrix_mapped.h5ad' \
--Pro_path='dataset/test/dataset2/GSM2685244_protein_3_PBMCs_matrix_mapped.h5ad'

# assign gpu
# add 'os.environ["CUDA_VISIBLE_DEVICES"]="0,1"' to 'stage3_fine-tune.py'

# nnodes:当前job包含多少个节点(单机多卡nnodes=1)
# node_rank: 当前节点的优先级
# nproc_per_node: 当前主机创建的进程数（一般设定为当前主机的GPU数量）
# master_addr: master节点的ip
# master_port: master节点的port


# performer_enc_dec.py  'self.pos_emb = nn.Embedding(85500,dim,padding_idx=0)'  85500 -> 300000
# mask 'model = torch.load(args.pretrain_checkpoint)' in 'stage3_fine-tune.py'
# utils.py: 'loss = F.mse_loss(y_hat[pro_mask], y[pro_mask])' -> 'loss = F.cross_entropy(y_hat[pro_mask], y[pro_mask])' (line 43)
#            'test_loss += F.mse_loss(y_hat[pro_mask], y[pro_mask]).item()' -> 'test_loss += F.cross_entropy(y_hat[pro_mask], y[pro_mask]).item()' (line 80)
#            'train_ccc += loss2(y_hat[pro_mask], y[pro_mask]).item()' -> 'auroc = AUROC(task='binary'); train_ccc += auroc(y_hat[pro_mask], y[pro_mask]).item()'  (line 46)
#            'test_ccc += loss2(y_hat[pro_mask], y[pro_mask]).item()' -> 'auroc = AUROC(task='binary'); test_ccc += auroc(y_hat[pro_mask], y[pro_mask]).item()'    (line 81)
#         add 'from torchmetrics import AUROC'  (line 7)
#         'loss' -> 'ce_loss' & 'ccc' -> 'auroc'
#         add 'print('Testing set: Average ce_loss: {:.4f}, Average auroc: {:.4f}'.format(test_loss, test_ccc), flush=True)' to line 84

# stage3_fine-tune.py: 'mse' -> 'cross entropy' & 'ccc' -> 'auroc' (line 234)
#                      'mse' -> 'cross entropy' & 'ccc' -> 'auroc' (line 235)
#                      add 'test_loss, test_ccc = test(model, device, test_loader)' to line 201

# pip install torchmetrics

# small datasets
python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node 1 --master_addr '10.11.41.108' --master_port 6123 \
code/stage3_fine-tune.py --batch_size 32 --test_batch_size 4 --translator_depth 1 --epoch=100 --seed 1 --frac_finetune_test 0.1 \
--fix_set --enc_max_seq_len 10000 --dec_max_seq_len 1000 --dim 64 --translator_depth 2 --enc_depth 2 --enc_heads 4 --dec_depth 2 --dec_heads 4 \
--RNA_path='rna2atac/rna_scTranslator.h5ad' --Pro_path='rna2atac/atac_scTranslator.h5ad'

# large datasets
python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node 1 --master_addr '10.11.41.108' --master_port 6123 \
code/stage3_fine-tune.py --batch_size 64 --test_batch_size 64 --translator_depth 1 --epoch=100 --seed 0 --frac_finetune_test 0.1 \
--enc_max_seq_len 50000 --dec_max_seq_len 1000 --dim 16 --translator_depth 1 --enc_depth 1 --enc_heads 1 --dec_depth 1 --dec_heads 1 \
--RNA_path='rna2atac/rna_scTranslator.h5ad' --Pro_path='rna2atac/atac_scTranslator.h5ad'


# cCREs: wget -c https://downloads.wenglab.org/V3/GRCh38-cCREs.bed
awk '{if($1!="chrX" && $1!="chrY") print($1 "\t" $2 "\t" $3)}' GRCh38-cCREs.bed > human_cCREs.bed  # 1,036,913

# conda install -c bioconda bedtools
# pip install pybedtools


################################################################################################################
## trans_atac_h5ad.py
import scanpy as sc
import pandas as pd
import numpy as np
import pybedtools
from tqdm import tqdm

atac = sc.read_h5ad('atac_scTranslator.h5ad')

peaks = pd.DataFrame({'id': atac.var_names})
peaks['chr'] = peaks['id'].map(lambda x: x.split(':')[0])
peaks['start'] = peaks['id'].map(lambda x: x.split(':')[1].split('-')[0])
peaks['end'] = peaks['id'].map(lambda x: x.split(':')[1].split('-')[1])
peaks.drop(columns='id', inplace=True)
peaks['idx'] = range(peaks.shape[0])

cCREs = pd.read_table('human_cCREs.bed', names=['chr', 'start', 'end'])
cCREs['idx'] = range(cCREs.shape[0])
cCREs_bed = pybedtools.BedTool.from_dataframe(cCREs)
peaks_bed = pybedtools.BedTool.from_dataframe(peaks)
idx_map = peaks_bed.intersect(cCREs_bed, wa=True, wb=True).to_dataframe().iloc[:, [3, 7]]
idx_map.columns = ['peaks_idx', 'cCREs_idx']

m = np.zeros([atac.n_obs, cCREs_bed.to_dataframe().shape[0]], dtype='float32')
for i in tqdm(range(atac.X.shape[0])):
    m[i][idx_map[idx_map['peaks_idx'].isin(peaks.iloc[np.nonzero(atac.X[i])]['idx'])]['cCREs_idx']] = 1

atac_new_var = pd.DataFrame({'gene_ids': cCREs['chr'] + ':' + cCREs['start'].map(str) + '-' + cCREs['end'].map(str), 'feature_types': 'Peaks', 'my_Id': range(cCREs.shape[0])})
atac_new = sc.AnnData(m, obs=atac.obs, var=atac_new_var)
atac_new.write('atac_scTranslator_new.h5ad')
################################################################################################################



# human gtf file: wget -c https://ftp.ensembl.org/pub/release-110/gtf/homo_sapiens/Homo_sapiens.GRCh38.110.chr.gtf.gz
# gunzip Homo_sapiens.GRCh38.110.chr.gtf.gz
grep gene_name Homo_sapiens.GRCh38.110.chr.gtf | awk -F "\t" '{if($1!="X" && $1!="Y" && $1!="MT" && $3=="gene") print$9}' | awk '{print $2}' | sed 's/"//g' | sed 's/;//g' > gene_id.txt
grep gene_name Homo_sapiens.GRCh38.110.chr.gtf | awk -F "\t" '{if($1!="X" && $1!="Y" && $1!="MT" && $3=="gene") print$9}' | awk '{print $6}' | sed 's/"//g' | sed 's/;//g' > gene_name.txt
paste gene_id.txt gene_name.txt > human_genes.txt  # 39884
rm gene_id.txt gene_name.txt

################################################################################################################
## trans_rna_h5ad.py
import scanpy as sc
import pandas as pd
import numpy as np

rna = sc.read_h5ad('rna_scTranslator.h5ad')
genes = pd.read_table('human_genes.txt', names=['gene_id', 'gene_name'])

rna_genes = rna.var.copy()
rna_genes['gene_id'] = rna_genes['gene_ids'].map(lambda x: x.split('.')[0])
rna_exp = pd.concat([rna_genes, pd.DataFrame(rna.X.T, index=rna_genes.index)], axis=1)

X_new = pd.merge(genes, rna_exp, how='left', on='gene_id').iloc[:, 5:].T
X_new = np.array(X_new, dtype='float32')
X_new[np.isnan(X_new)] = 0

rna_new_var = pd.DataFrame({'gene_ids': genes['gene_id'], 'feature_types': 'Gene Expression', 'my_Id': range(genes.shape[0])})
rna_new = sc.AnnData(X_new, obs=rna.obs, var=rna_new_var)
rna_new.write('rna_scTranslator_new.h5ad')
################################################################################################################



time python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node 1 --master_addr '10.11.41.108' --master_port 6123 \
code/stage3_fine-tune.py --batch_size 64 --test_batch_size 64 --lr 2e-3 --translator_depth 1 --epoch=100 --seed 0 --frac_finetune_test 0.1 \
--enc_max_seq_len 39884 --dec_max_seq_len 1036913 --dim 16 --translator_depth 1 --enc_depth 1 --enc_heads 1 --dec_depth 1 --dec_heads 1 \
--RNA_path='rna2atac/rna_scTranslator_new.h5ad' --Pro_path='rna2atac/atac_scTranslator_new.h5ad'

#pip install torcheval

time python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node 1 --master_addr '10.11.41.108' --master_port 6123 \
code/stage3_fine-tune.py --batch_size 128 --test_batch_size 64 --lr 1e-2 --translator_depth 1 --epoch=100 --seed 0 --frac_finetune_test 0.1 \
--enc_max_seq_len 39884 --dec_max_seq_len 1000 --dim 16 --translator_depth 1 --enc_depth 1 --enc_heads 1 --dec_depth 1 --dec_heads 1 \
--RNA_path='rna2atac/rna_scTranslator.h5ad' --Pro_path='rna2atac/atac_scTranslator.h5ad' --pretrain_checkpoint 'scM2M/epoch_2.pt'


from torchmetrics import MetricCollection, Accuracy, Precision, Recall
target = torch.tensor([0, 2, 0, 2, 0, 1, 0, 2])
preds = torch.tensor([2, 1, 2, 0, 1, 2, 2, 2])
metric_collection = MetricCollection([
    Accuracy(),
    Precision(num_classes=3, average='macro'),
    Recall(num_classes=3, average='macro')
])
print(metric_collection(preds, target))


## no weight (scTranslator)
time python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node 1 --master_addr '10.11.41.108' --master_port 6123 \
code/stage3_fine-tune.py --batch_size 64 --test_batch_size 64 --lr 2e-3 --translator_depth 1 --epoch=100 --seed 0 --frac_finetune_test 0.1 \
--enc_max_seq_len 39884 --dec_max_seq_len 1036913 --dim 16 --translator_depth 1 --enc_depth 1 --enc_heads 1 --dec_depth 1 --dec_heads 1 \
--RNA_path='rna2atac/rna_scTranslator_new.h5ad' --Pro_path='rna2atac/atac_scTranslator_new.h5ad' 

## with weight (scTranslator_with_weight)
time python -m torch.distributed.launch --nnodes=1 --node_rank=0 --nproc_per_node 1 --master_addr '10.11.41.108' --master_port 6124 \
code/stage3_fine-tune.py --batch_size 64 --test_batch_size 64 --lr 2e-3 --translator_depth 1 --epoch=100 --seed 0 --frac_finetune_test 0.1 \
--enc_max_seq_len 39884 --dec_max_seq_len 1036913 --dim 16 --translator_depth 1 --enc_depth 1 --enc_heads 1 --dec_depth 1 --dec_heads 1 \
--RNA_path='rna2atac/rna_scTranslator_new.h5ad' --Pro_path='rna2atac/atac_scTranslator_new.h5ad' 


time python code/stage3_inference_without_finetune.py --pretrain_checkpoint='scM2M_no_weight/epoch_5.pt' \
--RNA_path='rna2atac/rna_scTranslator_new.h5ad' --Pro_path='rna2atac/atac_scTranslator_new.h5ad' \
--test_batch_size 64 --seed 0 --enc_max_seq_len 39884 --dec_max_seq_len 1036913 



# --fix_set

# performer_enc_dec.py
# add 'enc_kwargs['max_seq_len']=128' to line 200
# add 'self.d_r = nn.Linear(50000, 128)'  to line 206
# 'seq_in' -> 'self.d_r(seq_in)'  &  'seq_inID' -> seq_inID[:, :128]
# add 'dec_kwargs['max_seq_len']=256' to line 205
# add 'self.d_r_2 = nn.Linear(256, 1000)' to line 206
# 'self.dec(seq_out, seq_outID, **dec_kwargs)' -> 'self.d_r_2(self.dec(seq_out, seq_outID[:, :256], **dec_kwargs))'  line 212


# utils.py
# add '[:, :128]' to line 29
# add '[:, :128]' to line 67
# add '[:, :256]' to line 35
# add '[:, :256]' to line 73
# add 'from tqdm import tqdm' to line 7
# 'enumerate(train_loader)' -> 'enumerate(tqdm(train_loader, desc='Training', ncols=100))' line 25
# 'test_loader' -> 'tqdm(test_loader, desc='Testing', ncols=100)' line 63


#seq_in shape: torch.Size([4, 50000])
#encodings shape: torch.Size([4, 50000, 16])
#seq_out shape: torch.Size([4, 1000, 16])
#out shape: torch.Size([4, 1000])

# input embedding!!!!!!!!!

#torch.squeeze(nn.Linear(16, 1)(seq_out))

i = torch.tensor([1., 2, 3, 4, 5, 6, 7, 8])
i = i.reshape([2, 1, 4])
o = nn.Linear(4, 1)(i)

seq_inID[:, :128]

torch.save(net, path) # 保存模型


# 50000 -> 50000 with decoder oom!
# 50000 -> 50000 without decoder just ok!





## data preprocess
import scanpy as sc
import numpy as np

rna = sc.read_h5ad('/fs/home/jiluzhang/Benchmark/sci-CAR/data/RNA.h5ad')
rna.var['mt'] = range(rna.var.shape[0])
rna.var = rna.var.rename(columns={'mt': 'my_Id'})
rna.X = np.array(rna.X.todense())  # convert compressed to array
rna.write('rna_scTranslator.h5ad')


atac = sc.read_h5ad('/fs/home/jiluzhang/Benchmark/sci-CAR/data/ATAC.h5ad')
atac.var['chrom'] = range(atac.var.shape[0])
atac.var = atac.var.rename(columns={'chrom': 'my_Id'})
atac.X = np.array(atac.X.todense())
atac.X[atac.X>1] = 1
atac.X[atac.X<1] = 0
atac.write('atac_scTranslator.h5ad')





