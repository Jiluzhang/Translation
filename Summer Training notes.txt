#### Day 3 ####
## 感知机
二分类模型
训练感知机等价于使用批量大小为1的梯度下降
感知机不能拟合XOR函数，它只能产生线性分割面

## 多层感知机
sigmoid & Tanh & ReLU激活函数
Softmax回归加入隐藏层后即多层感知机
多隐藏层  超参数（隐藏层数 & 每层隐藏层的大小）

## 多层感知机代码实现
一层感知机理论上可以拟合任意函数
浅层网络比深层网络实践上更难训练
激活函数相比网络层数对训练结果的影响小
神经网络最好不要设计成动态性（可能涉及实际问题）

## 模型选择
训练误差 & 泛化误差
验证数据集（评估模型好坏） & 测试数据集（只用一次，不能用于调参）
K-折交叉验证（常用K=5或10）常于非大数据集

## 过拟合和欠拟合
模型容量（拟合各种函数的能力）
估计模型容量（参数个数 & 参数值的选择范围）
VC dimension（一个最大的数据集的大小）
支持N维输入的感知机的VC维是N+1
一些多层感知机的VC维O(Nlog2N)

## Q&A
SVM处理大数据集速度较慢，且参数不太敏感
深度学习不太使用k-折交叉验证
超参数搜索（推荐经验导向或随机）

## 论文精读GAN
Generative Adversarial Nets
networks -> nets
GAN是无监督学习
用有监督学习的损失函数做无监督学习
可以训练模型对训练数据和测试数据进行区分



## Day 4
## 权重衰退
将当前权重进行缩小，再进行参数更新
通过限制参数值的选择范围控制模型容量
使用均方范数作为硬性限制
通常不限制偏移b
使用均方范数作为柔性限制
'weight_decay': wd  一般为1e-3（或1e-2）

## 丢弃法（dropout)
使用有噪音的数据等价于Tikhonov正则
丢弃法是在层之间加入噪音，是将一些输出项随机置0来控制模型复杂度
正则化只在训练模型过程使用
通常将丢弃法作用在隐藏全连接层的输出上
丢弃概率是控制模型复杂度的超参数（0.5, 0.9, 0.1）

## 丢弃法的实现
做乘法比选值赋值操作更快
通常会构建复杂的网络然后使用丢弃法控制复杂度，而不是直接使用较简单的网络而不使用丢弃法

## Q&A
深度学习的正确性无法保证，因为网络过于复杂
丢弃法用于全连接层，BN用于卷积网络
做预测时也可以使用丢弃法，但是会使输出结果产生随机性，可通过多次预测解决
改变数据标签也是一种正则化方式
Transformer可以视为一种kernal machine

## 数值稳定性
梯度爆炸 & 梯度消失
数值问题

## 让训练更加稳定
目标为让梯度值在合理的范围内
将乘法变为加法
归一化 & 梯度剪裁
合理的权重初始和激活函数
将每层的输出和梯度都看作随机变量，让它们的均值和方差都保持一致
在模型训练开始时更容易不稳定
Xavier权重初始化
调整Sigmoid为4*sigmoid(x)-2使得在零点附近呈现线性f(x)

## Q&A
数值压缩不会影响模型的准确性
孪生网络

## ResNet论文精读
short connections



























