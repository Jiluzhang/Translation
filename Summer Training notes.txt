#### Day 3 ####
## 感知机
二分类模型
训练感知机等价于使用批量大小为1的梯度下降
感知机不能拟合XOR函数，它只能产生线性分割面

## 多层感知机
sigmoid & Tanh & ReLU激活函数
Softmax回归加入隐藏层后即多层感知机
多隐藏层  超参数（隐藏层数 & 每层隐藏层的大小）

## 多层感知机代码实现
一层感知机理论上可以拟合任意函数
浅层网络比深层网络实践上更难训练
激活函数相比网络层数对训练结果的影响小
神经网络最好不要设计成动态性（可能涉及实际问题）

## 模型选择
训练误差 & 泛化误差
验证数据集（评估模型好坏） & 测试数据集（只用一次，不能用于调参）
K-折交叉验证（常用K=5或10）常于非大数据集

## 过拟合和欠拟合
模型容量（拟合各种函数的能力）
估计模型容量（参数个数 & 参数值的选择范围）
VC dimension（一个最大的数据集的大小）
支持N维输入的感知机的VC维是N+1
一些多层感知机的VC维O(Nlog2N)

## Q&A
SVM处理大数据集速度较慢，且参数不太敏感
深度学习不太使用k-折交叉验证
超参数搜索（推荐经验导向或随机）

## 论文精读GAN
Generative Adversarial Nets
networks -> nets
GAN是无监督学习
用有监督学习的损失函数做无监督学习
可以训练模型对训练数据和测试数据进行区分



## Day 4
## 权重衰退
将当前权重进行缩小，再进行参数更新
通过限制参数值的选择范围控制模型容量
使用均方范数作为硬性限制
通常不限制偏移b
使用均方范数作为柔性限制
'weight_decay': wd  一般为1e-3（或1e-2）

## 丢弃法（dropout)
使用有噪音的数据等价于Tikhonov正则
丢弃法是在层之间加入噪音，是将一些输出项随机置0来控制模型复杂度
正则化只在训练模型过程使用
通常将丢弃法作用在隐藏全连接层的输出上
丢弃概率是控制模型复杂度的超参数（0.5, 0.9, 0.1）

## 丢弃法的实现
做乘法比选值赋值操作更快
通常会构建复杂的网络然后使用丢弃法控制复杂度，而不是直接使用较简单的网络而不使用丢弃法

## Q&A
深度学习的正确性无法保证，因为网络过于复杂
丢弃法用于全连接层，BN用于卷积网络
做预测时也可以使用丢弃法，但是会使输出结果产生随机性，可通过多次预测解决
改变数据标签也是一种正则化方式
Transformer可以视为一种kernal machine

## 数值稳定性
梯度爆炸 & 梯度消失
数值问题

## 让训练更加稳定
目标为让梯度值在合理的范围内
将乘法变为加法
归一化 & 梯度剪裁
合理的权重初始和激活函数
将每层的输出和梯度都看作随机变量，让它们的均值和方差都保持一致
在模型训练开始时更容易不稳定
Xavier权重初始化
调整Sigmoid为4*sigmoid(x)-2使得在零点附近呈现线性f(x)

## Q&A
数值压缩不会影响模型的准确性
孪生网络

## ResNet论文精读
short connections

## Transformer论文精读（Attention is all you need）
batch normalization将列（特征）转化为均值为1，方差为0
layer normalization将行（样本）转化为均值为1，方差为0
RNN是把上一个时刻的信息输出传入下一个时刻作为输入
Transformer是通过attention层全局地获取整个序列的信息
restricted self-attention的计算限制在某个固定的范围
byte-pair encoding (BPE)是将词根提取，避免因时态等造成的词汇量太大
TPU适合于做大矩阵的乘法
label smoothing
归纳偏置（学习算法中，当学习器去预测其未遇到过的输入结果时，所做的一些假设的集合）(Mitchell, 1980)


## Day 5 ##
## 实战Kaggle比赛：预测房价
第一列特征为ID，需要删去
特征缩放为零均值和单位方差对数据进行标准化
将所有缺失的值替换为相应特征的平均值
独热编码处理离散值
处理大数值可以使用log(mse)，即更关心相对误差

## 房价预测（https://www.kaggle.com/c/california-house-prices/overview）

## Q&A
可以在相对小的数据集中训练模型选择超参数，然后在大数据集进行少量调试

## 层和块
nn.Sequential()
继承nn.Module（__init__需要哪些层 & 前向传播如何计算）

## 参数管理
访问参数
state_dict() -> 返回OrderedDict
.bias    .bias.data    .weight.grad
net.named_parameters()
print(net)
内置初始化
nn.init.normal_()  # "_"表示直接替换，而不是返回
nn.init.zero_()    nn.init.constant_()
net.apply()
m.weight.data *= m.weight.data.abs() >= 5  #
参数绑定（权重共享）
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(),
                                                shared, nn.ReLU(), nn.Linear(8, 1))
## 自定义层
继承nn.module
nn.Parameter() 把初始化的值包起来

## 读写文件
加载和保存张量(列表 & 字典)  torch.save()  torch.load()
加载和保存模型参数  
torch.save(net.state_dict(), 'mlp.params')
clone = MLP()
clone.load_state_dict(torch.load('mlo.params'))
clone.eval()

## Q&A
类别变量one-hot encoding维度过多，可用稀疏矩阵存储或者直接忽略
kaiming初始化
几乎不存在不可导的函数，可能存在某几个离散点不可导

## 使用GPU
nvidia-smi
torch.cuda.device('cuda')
torch.cuda.device('cuda:1')
torch.cuda.device_count()
X= torch.ones(2, 3, device=try_gpu())
保持张量在同一个GPU上，以保证计算性能
net.to(device=try_gpu())

## 购买GPU
内存似乎没那么重要（买最贵的，最新的-_-）
compute power & Memory size & Memory bandwith

## Q&A
GPU显存比CPU内存贵
GPU满负荷运行不要紧，关键是控制好温度不要太高
每创建一个实例，就会有自己的参数，把实例放在不同地方，才会共享参数

## BERT
双向 & 完形填空


## Day 6 ##
## 房价预测总结
集成学习
特征预处理和超参数是取得好成绩的基础
automl

## Q&A
观察超参数附近模型的稳定性可以感知是否过拟合
神经网络搜索（Neural Architecture Search, NAS）一种自动设计神经网络的技术
AutoGluon（一个AutoML的开源库）

## 从全连接到卷积
平移不变性 & 局部性

## 卷积层
二维交叉相关和卷积得到的结果左右和上下相反，但在实际使用中没有区别
边缘检测 & 锐化 & 高斯模糊
一维交叉相关（文本，语言，时序序列）
三维交叉相关（视频，医学图像，气象地图）
卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出
核矩阵和偏移是可学习的参数
核矩阵的大小是超参数

## 卷积的代码实现
conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)

## Q&A
卷积核维度大一些，网络深一些，比卷积核维度小一点，网络浅一点，效果更好

## 填充和步幅
更大的卷积核可以更快地减小输出大小
填充：在输入周围添加额外的行和列
一般不使用大小为偶数的卷积核
步幅：行/列的滑动步长

## 填充和步幅的代码实现
conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))

## Q&A
核大小是最关键的超参数
填充通常设为核大小减1
步幅通常设为1，设为大于1是因为计算量过大
机器学习可以视为压缩算法

## 多输入多输出通道
每个通道都有一个卷积核，结果是所有通道卷积结果的和
可以有多个三维卷积核，每个核生成一个输出通道
每个输出通道可以识别特定模式
输入通道核识别并组合输入中的模式
1*1卷积层不识别空间模式，只是融合通道

## 多输入多输出通道代码实现

## Q&A
padding为0基本不会影响模型性能
通道之间不共享参数
feature map是卷积的输出

## 池化层
卷积对位置敏感
需要保持一定程度的平移不变性（缓解位置敏感性）
二维最大池化返回滑动窗口中的最大值
池化层与卷积层类似，都具有填充和步幅
没有可学习的参数
在每个输入通道应用池化层以获取相应的输出通道
输出通道数=输入通道数
平均池化层

## 池化层的代码实现
nn.MaxPool2d(3, padding=1, stride=2)
nn.MaxPool2d((2, 3), padding=(1, 1), stride=(2, 3))
pytorch中的步幅与池化窗口的大小相同

## Q&A
池化层一般放在卷积层的后面
可通过列表操作得到行数或列数不确定的矩阵
池化层用的越来越少的原因：通过设置stride减少计算量 & 数据增强

## GPT论文精读
Hacker News
Github Copilot
GPT(Generative Pre-Training)
...

## ViT论文精读
vision transformer
遮挡 & 数据分布偏移 & 对抗性patch & 随机打乱
用特征图当做Transformer的输入
...


## Day 7 ##
## LeNet
手写的数字识别
MNIST(50,000个训练数据 & 10,000个测试数据 & 图像大小28×28 & 10类)
2个卷积层 & 2个池化层 & 2个全连接层
nn.Conv2d() -> nn.AvgPool2d() -> nn.Conv2d() -> nn.AvgPool2d() -> nn.Linear() -> nn.Linear() -> nn.Linear()
Fashion-MNIST数据集

## LeNet代码实现
...

## Q&A
每个输出通道匹配某些特征
max池化比average值更大，会更好训练些
Lua语言（LeNet最初实现用的语言）
准确率在实践中很大程度上取决于用户体验和承受的阈值
CNN Explainer

## AlexNet
Learning with Kernels 2001
特征提取  选择核函数  凸优化问题  漂亮的定理
Multiple View Geometry in computer vision
抽取特征  描述几何  （非）凸优化  漂亮定理
特征工程  特征描述子：SIFT，SURF
数据增长 ~ 计算能力
ImageNet 2010
AlexNet本质上是更深更大的LeNet（主要改进：丢弃法 & ReLU & MaxPooling & 数据增强）
图片 -> 人工特征提取 -> SVM
图片 -> 通过CNN学习特征 -> Softmax回归

## AlexNet代码实现
AlexNet的学习率比LeNet低

## Q&A
AlexNet抽取的特征一般不符合人类的逻辑，很大原因是其最终目的是为了得到准确的分类
local response normalization (LRN) 在后续被认为效果不好
数据增强后模型变差是正常的-_-

## VGG
AlexNet网络形状不规则，无法详细解释设计思路
VGG可视为更大更深的AlexNet
将卷积层组合成块
VGG块  3*3卷积（填充1）（n层，m通道） 2*2最大池化层（步幅2）
VGG-16, VGG-19
不同的卷积块个数和超参数可以得到不同复杂度的变种（低配版 & 高配版）

## VGG代码实现
num_convs, in_channels, out_channels

## Q&A
不要过度设计模型

## 网络中的网络NiN
全连接层过大，导致模型的参数主要集中在全连接层
NiN块  一个卷积层后跟两个全连接层（步幅为1，无填充，输出形状和卷积层输出一样）
无全连接层 & 交替使用NiN块和步幅为2的最大池化层 & 最后使用全局平均池化层得到输出
模型设计基于AlexNet

## NiN的代码实现
in_channels, out_channels, kernel_size, strides, padding
nn.AdaptiveAvgPool2d() 全局池化

## Q&A
宽的全连接层容易过拟合
GPU显存-内存泄露
全局池化层可以降低模型复杂度，但是模型收敛会变慢
pytorch会自动初始化参数


















