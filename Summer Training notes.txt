#### Day 3 ####
## 感知机
二分类模型
训练感知机等价于使用批量大小为1的梯度下降
感知机不能拟合XOR函数，它只能产生线性分割面

## 多层感知机
sigmoid & Tanh & ReLU激活函数
Softmax回归加入隐藏层后即多层感知机
多隐藏层  超参数（隐藏层数 & 每层隐藏层的大小）

## 多层感知机代码实现
一层感知机理论上可以拟合任意函数
浅层网络比深层网络实践上更难训练
激活函数相比网络层数对训练结果的影响小
神经网络最好不要设计成动态性（可能涉及实际问题）

## 模型选择
训练误差 & 泛化误差
验证数据集（评估模型好坏） & 测试数据集（只用一次，不能用于调参）
K-折交叉验证（常用K=5或10）常于非大数据集

## 过拟合和欠拟合
模型容量（拟合各种函数的能力）
估计模型容量（参数个数 & 参数值的选择范围）
VC dimension（一个最大的数据集的大小）
支持N维输入的感知机的VC维是N+1
一些多层感知机的VC维O(Nlog2N)

## Q&A
SVM处理大数据集速度较慢，且参数不太敏感
深度学习不太使用k-折交叉验证
超参数搜索（推荐经验导向或随机）

## 论文精读GAN
Generative Adversarial Nets
networks -> nets
GAN是无监督学习
用有监督学习的损失函数做无监督学习
可以训练模型对训练数据和测试数据进行区分



## Day 4
## 权重衰退
将当前权重进行缩小，再进行参数更新
通过限制参数值的选择范围控制模型容量
使用均方范数作为硬性限制
通常不限制偏移b
使用均方范数作为柔性限制
'weight_decay': wd  一般为1e-3（或1e-2）

## 丢弃法（dropout)
使用有噪音的数据等价于Tikhonov正则
丢弃法是在层之间加入噪音，是将一些输出项随机置0来控制模型复杂度
正则化只在训练模型过程使用
通常将丢弃法作用在隐藏全连接层的输出上
丢弃概率是控制模型复杂度的超参数（0.5, 0.9, 0.1）

## 丢弃法的实现
做乘法比选值赋值操作更快
通常会构建复杂的网络然后使用丢弃法控制复杂度，而不是直接使用较简单的网络而不使用丢弃法

## Q&A
深度学习的正确性无法保证，因为网络过于复杂
丢弃法用于全连接层，BN用于卷积网络
做预测时也可以使用丢弃法，但是会使输出结果产生随机性，可通过多次预测解决
改变数据标签也是一种正则化方式
Transformer可以视为一种kernal machine

## 数值稳定性
梯度爆炸 & 梯度消失
数值问题

## 让训练更加稳定
目标为让梯度值在合理的范围内
将乘法变为加法
归一化 & 梯度剪裁
合理的权重初始和激活函数
将每层的输出和梯度都看作随机变量，让它们的均值和方差都保持一致
在模型训练开始时更容易不稳定
Xavier权重初始化
调整Sigmoid为4*sigmoid(x)-2使得在零点附近呈现线性f(x)

## Q&A
数值压缩不会影响模型的准确性
孪生网络

## ResNet论文精读
short connections

## Transformer论文精读（Attention is all you need）
batch normalization将列（特征）转化为均值为1，方差为0
layer normalization将行（样本）转化为均值为1，方差为0
RNN是把上一个时刻的信息输出传入下一个时刻作为输入
Transformer是通过attention层全局地获取整个序列的信息
restricted self-attention的计算限制在某个固定的范围
byte-pair encoding (BPE)是将词根提取，避免因时态等造成的词汇量太大
TPU适合于做大矩阵的乘法
label smoothing
归纳偏置（学习算法中，当学习器去预测其未遇到过的输入结果时，所做的一些假设的集合）(Mitchell, 1980)


## Day 5 ##
## 实战Kaggle比赛：预测房价
第一列特征为ID，需要删去
特征缩放为零均值和单位方差对数据进行标准化
将所有缺失的值替换为相应特征的平均值
独热编码处理离散值
处理大数值可以使用log(mse)，即更关心相对误差

## 房价预测（https://www.kaggle.com/c/california-house-prices/overview）

## Q&A
可以在相对小的数据集中训练模型选择超参数，然后在大数据集进行少量调试

## 层和块
nn.Sequential()
继承nn.Module（__init__需要哪些层 & 前向传播如何计算）

## 参数管理
访问参数
state_dict() -> 返回OrderedDict
.bias    .bias.data    .weight.grad
net.named_parameters()
print(net)
内置初始化
nn.init.normal_()  # "_"表示直接替换，而不是返回
nn.init.zero_()    nn.init.constant_()
net.apply()
m.weight.data *= m.weight.data.abs() >= 5  #
参数绑定（权重共享）
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(),
                                                shared, nn.ReLU(), nn.Linear(8, 1))
## 自定义层
继承nn.module
nn.Parameter() 把初始化的值包起来

## 读写文件
加载和保存张量(列表 & 字典)  torch.save()  torch.load()
加载和保存模型参数  
torch.save(net.state_dict(), 'mlp.params')
clone = MLP()
clone.load_state_dict(torch.load('mlo.params'))
clone.eval()

## Q&A
类别变量one-hot encoding维度过多，可用稀疏矩阵存储或者直接忽略
kaiming初始化
几乎不存在不可导的函数，可能存在某几个离散点不可导

## 使用GPU
nvidia-smi
torch.cuda.device('cuda')
torch.cuda.device('cuda:1')
torch.cuda.device_count()
X= torch.ones(2, 3, device=try_gpu())
保持张量在同一个GPU上，以保证计算性能
net.to(device=try_gpu())

## 购买GPU
内存似乎没那么重要（买最贵的，最新的-_-）
compute power & Memory size & Memory bandwith

## Q&A
GPU显存比CPU内存贵
GPU满负荷运行不要紧，关键是控制好温度不要太高
每创建一个实例，就会有自己的参数，把实例放在不同地方，才会共享参数

## BERT
双向 & 完形填空















