document.querySelector('video').playbackRate=2.25

可以访问内网的同学可以通过 http://10.11.41.108/ 提交作业，其他同学可以将作业发送到biotraining@163.com ，请将作业跑完所有结果并保存成html，并且按照 编号_姓名_homework1.html 的格式命名后上传。编号请查询：https://www.kdocs.cn/l/ctmbSlF1c71i

#### Day 3 ####
## 感知机
二分类模型
训练感知机等价于使用批量大小为1的梯度下降
感知机不能拟合XOR函数，它只能产生线性分割面

## 多层感知机
sigmoid & Tanh & ReLU激活函数
Softmax回归加入隐藏层后即多层感知机
多隐藏层  超参数（隐藏层数 & 每层隐藏层的大小）

## 多层感知机代码实现
一层感知机理论上可以拟合任意函数
浅层网络比深层网络实践上更难训练
激活函数相比网络层数对训练结果的影响小
神经网络最好不要设计成动态性（可能涉及实际问题）

## 模型选择
训练误差 & 泛化误差
验证数据集（评估模型好坏） & 测试数据集（只用一次，不能用于调参）
K-折交叉验证（常用K=5或10）常于非大数据集

## 过拟合和欠拟合
模型容量（拟合各种函数的能力）
估计模型容量（参数个数 & 参数值的选择范围）
VC dimension（一个最大的数据集的大小）
支持N维输入的感知机的VC维是N+1
一些多层感知机的VC维O(Nlog2N)

## Q&A
SVM处理大数据集速度较慢，且参数不太敏感
深度学习不太使用k-折交叉验证
超参数搜索（推荐经验导向或随机）

## 论文精读GAN
Generative Adversarial Nets
networks -> nets
GAN是无监督学习
用有监督学习的损失函数做无监督学习
可以训练模型对训练数据和测试数据进行区分



## Day 4
## 权重衰退
将当前权重进行缩小，再进行参数更新
通过限制参数值的选择范围控制模型容量
使用均方范数作为硬性限制
通常不限制偏移b
使用均方范数作为柔性限制
'weight_decay': wd  一般为1e-3（或1e-2）

## 丢弃法（dropout)
使用有噪音的数据等价于Tikhonov正则
丢弃法是在层之间加入噪音，是将一些输出项随机置0来控制模型复杂度
正则化只在训练模型过程使用
通常将丢弃法作用在隐藏全连接层的输出上
丢弃概率是控制模型复杂度的超参数（0.5, 0.9, 0.1）

## 丢弃法的实现
做乘法比选值赋值操作更快
通常会构建复杂的网络然后使用丢弃法控制复杂度，而不是直接使用较简单的网络而不使用丢弃法

## Q&A
深度学习的正确性无法保证，因为网络过于复杂
丢弃法用于全连接层，BN用于卷积网络
做预测时也可以使用丢弃法，但是会使输出结果产生随机性，可通过多次预测解决
改变数据标签也是一种正则化方式
Transformer可以视为一种kernal machine

## 数值稳定性
梯度爆炸 & 梯度消失
数值问题

## 让训练更加稳定
目标为让梯度值在合理的范围内
将乘法变为加法
归一化 & 梯度剪裁
合理的权重初始和激活函数
将每层的输出和梯度都看作随机变量，让它们的均值和方差都保持一致
在模型训练开始时更容易不稳定
Xavier权重初始化
调整Sigmoid为4*sigmoid(x)-2使得在零点附近呈现线性f(x)

## Q&A
数值压缩不会影响模型的准确性
孪生网络

## ResNet论文精读
short connections

## Transformer论文精读（Attention is all you need）
batch normalization将列（特征）转化为均值为1，方差为0
layer normalization将行（样本）转化为均值为1，方差为0
RNN是把上一个时刻的信息输出传入下一个时刻作为输入
Transformer是通过attention层全局地获取整个序列的信息
restricted self-attention的计算限制在某个固定的范围
byte-pair encoding (BPE)是将词根提取，避免因时态等造成的词汇量太大
TPU适合于做大矩阵的乘法
label smoothing
归纳偏置（学习算法中，当学习器去预测其未遇到过的输入结果时，所做的一些假设的集合）(Mitchell, 1980)


## Day 5 ##
## 实战Kaggle比赛：预测房价
第一列特征为ID，需要删去
特征缩放为零均值和单位方差对数据进行标准化
将所有缺失的值替换为相应特征的平均值
独热编码处理离散值
处理大数值可以使用log(mse)，即更关心相对误差

## 房价预测（https://www.kaggle.com/c/california-house-prices/overview）

## Q&A
可以在相对小的数据集中训练模型选择超参数，然后在大数据集进行少量调试

## 层和块
nn.Sequential()
继承nn.Module（__init__需要哪些层 & 前向传播如何计算）

## 参数管理
访问参数
state_dict() -> 返回OrderedDict
.bias    .bias.data    .weight.grad
net.named_parameters()
print(net)
内置初始化
nn.init.normal_()  # "_"表示直接替换，而不是返回
nn.init.zero_()    nn.init.constant_()
net.apply()
m.weight.data *= m.weight.data.abs() >= 5  #
参数绑定（权重共享）
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(),
                                                shared, nn.ReLU(), nn.Linear(8, 1))
## 自定义层
继承nn.module
nn.Parameter() 把初始化的值包起来

## 读写文件
加载和保存张量(列表 & 字典)  torch.save()  torch.load()
加载和保存模型参数  
torch.save(net.state_dict(), 'mlp.params')
clone = MLP()
clone.load_state_dict(torch.load('mlo.params'))
clone.eval()

## Q&A
类别变量one-hot encoding维度过多，可用稀疏矩阵存储或者直接忽略
kaiming初始化
几乎不存在不可导的函数，可能存在某几个离散点不可导

## 使用GPU
nvidia-smi
torch.cuda.device('cuda')
torch.cuda.device('cuda:1')
torch.cuda.device_count()
X= torch.ones(2, 3, device=try_gpu())
保持张量在同一个GPU上，以保证计算性能
net.to(device=try_gpu())

## 购买GPU
内存似乎没那么重要（买最贵的，最新的-_-）
compute power & Memory size & Memory bandwith

## Q&A
GPU显存比CPU内存贵
GPU满负荷运行不要紧，关键是控制好温度不要太高
每创建一个实例，就会有自己的参数，把实例放在不同地方，才会共享参数

## BERT
双向 & 完形填空


## Day 6 ##
## 房价预测总结
集成学习
特征预处理和超参数是取得好成绩的基础
automl

## Q&A
观察超参数附近模型的稳定性可以感知是否过拟合
神经网络搜索（Neural Architecture Search, NAS）一种自动设计神经网络的技术
AutoGluon（一个AutoML的开源库）

## 从全连接到卷积
平移不变性 & 局部性

## 卷积层
二维交叉相关和卷积得到的结果左右和上下相反，但在实际使用中没有区别
边缘检测 & 锐化 & 高斯模糊
一维交叉相关（文本，语言，时序序列）
三维交叉相关（视频，医学图像，气象地图）
卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出
核矩阵和偏移是可学习的参数
核矩阵的大小是超参数

## 卷积的代码实现
conv2d = nn.Conv2d(1, 1, kernel_size=(1, 2), bias=False)

## Q&A
卷积核维度大一些，网络深一些，比卷积核维度小一点，网络浅一点，效果更好

## 填充和步幅
更大的卷积核可以更快地减小输出大小
填充：在输入周围添加额外的行和列
一般不使用大小为偶数的卷积核
步幅：行/列的滑动步长

## 填充和步幅的代码实现
conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))

## Q&A
核大小是最关键的超参数
填充通常设为核大小减1
步幅通常设为1，设为大于1是因为计算量过大
机器学习可以视为压缩算法

## 多输入多输出通道
每个通道都有一个卷积核，结果是所有通道卷积结果的和
可以有多个三维卷积核，每个核生成一个输出通道
每个输出通道可以识别特定模式
输入通道核识别并组合输入中的模式
1*1卷积层不识别空间模式，只是融合通道

## 多输入多输出通道代码实现

## Q&A
padding为0基本不会影响模型性能
通道之间不共享参数
feature map是卷积的输出

## 池化层
卷积对位置敏感
需要保持一定程度的平移不变性（缓解位置敏感性）
二维最大池化返回滑动窗口中的最大值
池化层与卷积层类似，都具有填充和步幅
没有可学习的参数
在每个输入通道应用池化层以获取相应的输出通道
输出通道数=输入通道数
平均池化层

## 池化层的代码实现
nn.MaxPool2d(3, padding=1, stride=2)
nn.MaxPool2d((2, 3), padding=(1, 1), stride=(2, 3))
pytorch中的步幅与池化窗口的大小相同

## Q&A
池化层一般放在卷积层的后面
可通过列表操作得到行数或列数不确定的矩阵
池化层用的越来越少的原因：通过设置stride减少计算量 & 数据增强

## GPT论文精读
Hacker News
Github Copilot
GPT(Generative Pre-Training)
...

## ViT论文精读
vision transformer
遮挡 & 数据分布偏移 & 对抗性patch & 随机打乱
用特征图当做Transformer的输入
...


## Day 7 ##
## LeNet
手写的数字识别
MNIST(50,000个训练数据 & 10,000个测试数据 & 图像大小28×28 & 10类)
2个卷积层 & 2个池化层 & 2个全连接层
nn.Conv2d() -> nn.AvgPool2d() -> nn.Conv2d() -> nn.AvgPool2d() -> nn.Linear() -> nn.Linear() -> nn.Linear()
Fashion-MNIST数据集

## LeNet代码实现
...

## Q&A
每个输出通道匹配某些特征
max池化比average值更大，会更好训练些
Lua语言（LeNet最初实现用的语言）
准确率在实践中很大程度上取决于用户体验和承受的阈值
CNN Explainer

## AlexNet
Learning with Kernels 2001
特征提取  选择核函数  凸优化问题  漂亮的定理
Multiple View Geometry in computer vision
抽取特征  描述几何  （非）凸优化  漂亮定理
特征工程  特征描述子：SIFT，SURF
数据增长 ~ 计算能力
ImageNet 2010
AlexNet本质上是更深更大的LeNet（主要改进：丢弃法 & ReLU & MaxPooling & 数据增强）
图片 -> 人工特征提取 -> SVM
图片 -> 通过CNN学习特征 -> Softmax回归

## AlexNet代码实现
AlexNet的学习率比LeNet低

## Q&A
AlexNet抽取的特征一般不符合人类的逻辑，很大原因是其最终目的是为了得到准确的分类
local response normalization (LRN) 在后续被认为效果不好
数据增强后模型变差是正常的-_-

## VGG
AlexNet网络形状不规则，无法详细解释设计思路
VGG可视为更大更深的AlexNet
将卷积层组合成块
VGG块  3*3卷积（填充1）（n层，m通道） 2*2最大池化层（步幅2）
VGG-16, VGG-19
不同的卷积块个数和超参数可以得到不同复杂度的变种（低配版 & 高配版）

## VGG代码实现
num_convs, in_channels, out_channels

## Q&A
不要过度设计模型

## 网络中的网络NiN
全连接层过大，导致模型的参数主要集中在全连接层
NiN块  一个卷积层后跟两个全连接层（步幅为1，无填充，输出形状和卷积层输出一样）
无全连接层 & 交替使用NiN块和步幅为2的最大池化层 & 最后使用全局平均池化层得到输出
模型设计基于AlexNet

## NiN的代码实现
in_channels, out_channels, kernel_size, strides, padding
nn.AdaptiveAvgPool2d() 全局池化

## Q&A
宽的全连接层容易过拟合
GPU显存-内存泄露
全局池化层可以降低模型复杂度，但是模型收敛会变慢
pytorch会自动初始化参数

## MAE论文精读
MAE可视为BERT的CV版本
非对称编码器和解码器指两者看到的信息不一致
MAE对数据增强不是太敏感
调整网络所有参数比只调整最后一层效果更好

## 如何找研究想法 1
打补丁法
选取比较新的论文

## 如何判断研究工作的价值
用有新意的方法有效的解决一个研究问题
新意度*有效性*问题大小=价值

## 论文novelty
Michael J. Black
beauty!!!
简单有效！！！
If you hear a good idea, there is a moment of surprise and then, the better it is, the more obivous it may seem.
用简单模型替代复杂模型获得可解释性
新意度！=复杂度、困难度、惊讶度、技术新意度、有效性


## Day 8 ##
## 含并行连结的网络（GoogLeNet）
GoogleNet使用了9个Inception块，是第一个达到上百层的网络（并行）
Inception块  4个路径从不同层面抽取信息，然后再输出通道维合并
高宽不变，通道数改变
和单3*3或5*5卷积层相比，Inception块有更少的参数个数和计算复杂度
Inception-BN (v2) 使用batch normalization
Inception-V3 修改了Inception块 替换卷积层
Inception-V4 使用残差连接

## GoogLeNet代码实现
...

## Q&A
DIVE INTO DEEP LEARNING (D2L)
ResNeSt: Split-Attention Networks

## 批量归一化
损伤出现在最后，后面的层训练较快
固定小批量的均值和方差
可学习的参数为γ（拉伸参数）和β（偏移参数）
对于全连接层，作用在特征维
对于卷积层，作用在通道维
批量归一化可能就是通过在每个小批量里加入噪音来控制模型复杂度，因此没必要和丢弃法一起用
批量归一化可以加速收敛，但一般不改变模型精读

## 批量归一化代码实现
eps=1e-5  momentum=0.9
批量归一化要加在激活函数前

## Q&A
马毅 深度学习第一性原理 白盒理论解释深度学习
layer normalization

## 残差网络（ResNet）
残差块  f(x)=x+g(x)
残差块使得很深的网络更加容易训练，甚至可以训练一千层的网络

## ResNet代码实现
...

## Q&A
cos学习率
数据增强可能导致测试精度大于训练精度

## ResNet为什么能训练出1000层的模型
将梯度乘法转化为加法！

## Q&A
可以根据所在层数设置不同的学习率，但还是比较难确定学习率的值范围

## 图片分类竞赛
176个叶子种类

## MoCo论文精读
对比学习  将相似的图片在语义空间上尽可能邻近，不相似的尽可能远离
自监督模型
instance dismination  每个样本自成一类
NCE: noise contrastive estimation
end-to-end  memory bank  MoCo


## Day 9 ##
## 深度学习硬件：CPU和GPU
数值计算步骤：主内存 -> L3 -> L2 -> L1 -> 寄存器
提升空间和时间的内存本地性
时间：重用数据使得保持它们在缓存里
空间：按序读写数据使得可以预读取
如果一个矩阵是按列存储，访问一行会比访问一列要快
超线程不一定能提升性能，因为它们共享寄存器
GPU的核数和带宽多于CPU，但控制流较弱
提升GPU利用率：并行（使用数千个线程） 内存本地性（缓存更小，架构更简单） 少用控制语句（支持有限，同步开销很大）
不要频繁地在CPU和GPU之间传数据
高性能计算编程
CPU: C++或任何高性能语言
GPU: Nvidia上用CUDA  其它用OpenCL

## Q&A
增加数据是提高泛化性最简单有效的方式

## 深度学习硬件：TPU和其它
DSP 数字信号处理  为数字信号处理算法设计：点积，卷积，FFT
FPGA 可编程阵列  有大量可以编程逻辑单元和可配置的连接
AI ASIC  Google TPU
Systolic Array  计算单元（PE）阵列  特别适合做矩阵乘法

## Q&A
专用性越高的硬件计算性能越高
开发生态  研究人员生态
编译器

## 单机多卡并行
在训练和预测时，我们将一个小批量计算切分到多个GPU上来达到加速的目的
常用的切分方案：数据并行  模型并行  通道并行
数据并行：读一个数据块 -> 拿回参数 -> 计算梯度 -> 发出梯度 -> 更新梯度
当模型很大时，会使用模型并行

## Q&A
模型并行的性能比数据并行一般会低

## 多GPU训练实现（从零开始）
allreduce函数将所有向量相加，并将结果广播给所有GPU
nn.parallel.scatter() 均匀切分数据并分发给GPU
当GPU数增加时，可以同时增加batch size以及learning rate！

## 多GPU训练实现（简洁实现）
nn.DataParallel(net, device_ids=devices)
batch size过大会降低模型精度

## Q&A
batch normalization ~ 模型精度？
GPU显存优化

## Swin Transformer论文精读
paperswithcode网站  可查看模型在数据集中的表现
swin transformer让vision transformer像卷积神经网络一样能分成几个block，能做层级式的特征提取，使提取的特征有多尺度的概念
FPN
UNet
多尺度特征
patch merging: 空间大小减半，通道数加倍
test time augmentation
密集型预测任务


## Day 8 ##
## 分布式训练
单机多卡
数据放在分布式文件系统上 & 多个worker & 多个参数服务器
尽量少在机器间做通讯
每个服务器对梯度求和，并更新参数
同步SGD：每个worker都是同步计算一个批量
增加每个GPU的批量大小，会导致系统性能变好，但是训练有效性会降低
使用一个大数据集 & 好的GPU-GPU和机器-机器带宽 & 高效的数据读取和预处理 & 模型需要有好的计算通讯比 & 
使用足够大的批量大小得到好的系统性能 & 使用高效的优化算法对对应大批量大小
更复杂的分布式有异步、模型并行

## Q&A
forward无法并行，backward可以一定程度做到并行
batch size尽量不超过10*类别数

## 数据增广
增加一个已有数据集，使得有更多的多样性
在语言里加入不同的背景噪音
改变图片的颜色和形状
在线生成 & 随机增强
翻转 & 切割（随机高宽比，随机大小，随机位置） & 颜色（色调，饱和度，明亮度）
https://github.com/aleju/imgaug
根据测试集数据的多样性选择数据增强的方式

## 代码实现
torchvision.transforms.RandomHorizontalFlip()   水平翻转
torchvision.transforms.RandomticalFlip()        上下翻转
torchvision.transforms.RandomResizedCrop()      随机剪裁
torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)     
随机更改图像的亮度（brightness）、对比度（contrast）、饱和度（saturation）、色调（hue）
结合多种图像增广方法
数据增广的计算量不小，尽量将num_workers设置的大一些
如果训练集的数据多样性远大于测试集，可能会出现训练集的精度小于测试集

## Q&A
原始样本需要多样性大，而不是仅仅是数量多
有对图片进行拼接的操作，此时label也需要拼接
多张图片叠加 mixup 加权平均
数据增广可以理解为不改变均值，但是增大了方差

## 微调
迁移学习
一个神经网络一般可以分为2块：特征提取 & 线性分类
特征提取拷贝预训练模型的参数，线性分类层随机初始化
使用更小的学习率 & 使用更少的数据迭代
源数据集远复杂于目标数据，通常微调效果更好
重用分类器权重 & 固定一些层（通常是底部层）

## 代码实现
热狗数据集
pretrained_net = torchvision.models.resnet18(pretrained=True)
pretrained_net.fc

## Q&A
数据不平衡对分类层比特征提取层影响更大
做微调时，源数据集最好与目标数据集尽量相似
微调对学习率不太敏感

## 第二次竞赛 树叶分类结果
0.98x~0.99x

## 实战Kaggle比赛：图像分类（CIFAR-10）
import shutil
drop_last=True
lr_period, lr_decay  每隔几个epoch将learning rate减少

## Q&A
weight decay是统计模型上的参数，lr decay是优化算法上的参数
SGD这种优化算法效果较好，可理解为是一种正则化

## 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）
torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
预训练模型微调 固定模型参数
top1 & top5 accuracy

## Q&A
某个方法不work，很多时候是没执行到位-_-

## AlphaFold 2论文精读
把结果换算成大家可以理解的概念，而不是纯粹数字的概念
特征提取 & 编码器 & 解码器
MSA：multi sequence alignment
genetic database search & pairing & structure database search
Evoformer是Transformer的变种
recycling (three times)
MSA row-wise gated self-attention with pair bias
MSA column-wise gated self-attention
氨基酸的相对位置
IPA: Invariant point attention

## 大模型时代下做科研的四个思路
1. efficient (PEFT: paramater efficient fine tuning)
2. pretrained model  new directions
3. plug-and-play
4. dataset, evaluation, survey
...

## 跟读者建立联系（研究的艺术一）
知道你的读者是谁？
we write to remember more accurately, understand better, and evaluate what we think more objectively.
I've found some new and interesting information
I've found a solution to an important practical problem
I've found an answer to an important question


## Day 11 ##
## 物体检测和数据集
目标检测  找到目标的位置
边缘框  通过4个数字定义  
(左上x，左上y，右下x，右下y)  （左上x，左上y，宽，高）
目标检测的数据集比图片分类数据集小很多
图片文件名，物体类别，边缘框
COCO  cocodataset.org  80类物体  330k图片  1.5M个物体

## 目标检测和边界框代码实现
box_corner_to_center()
box_center_to_corner()
plt支持（左上x，左上y，宽，高）这种模式

## 目标检测数据集
构建小型数据集  banana-detection
边框是按像素存储，还是按0-1存储

## Q&A
opencv提供很多传统算法
自己标注数据可以先小批量标注，然后用预训练模型进行微调预测，再将置信度较低的数据进行人工标记
color=['w']表示边缘框是白色

## 锚框
基于锚框的算法是目前的主流算法
预测每个锚框里是否含有关注的物体
预测锚框到真实边缘框的偏移
IoU 交并比 0表示无重叠，1表示重合
赋予锚框标号
每个锚框是一个训练样本
将每个锚框不是标注成背景，就是关联上一个真实的边缘框
会生成大量的锚框，进而导致产生大量的负类样本
使用非极大值抑制（NMS）输出
合并相似的预测
选中非背景类的最大预测值 -> 去掉所有其它和它IoU大于阈值的预测 -> 重复直到所有预测要么被选中，要么被去掉

## 锚框代码实现
生成以每个像素为中心具有不同形状的锚框
生成高质量的锚框很重要
对锚框偏移量的转换
nms()

## Q&A
分类有置信度，而回归没有置信度

## CLIP论文精读
zero-shot：没有在特定数据集中训练，直接进行预测
prompt template
prompt engineering & prompt ensemble
摆脱了categorical label的限制
大数据+大模型
泛化性能好于有监督学习模型
基于对比学习的训练很高效！
“How to Train Really Large Models on Many GPUs” from a staff in OPENAI

## Day 12 ##
## 树叶分类竞赛技术总结
数据增强，在测试时多次使用稍弱的增强，然后取平均
单一模型训练多次，然后取平均
训练算法和学习率
清理数据
跨图片增强  Mixup（随机叠加两张图片） CutMix（随机组合来自不同图片的块）
模型多维ResNet变种 DenseNet, ResNeXt, ResNeSt, EfficientNet
学习率一般是cosin或者训练不动时往下调
AutoGluon更多关注在工业界的应用，而非比赛
工业界通常固定模型超参数，将精力主要花在提升数据质量上

## 物体检测算法
R-CNN
使用启发式搜索算法选择锚框
使用预训练模型对每个锚框抽取特征
训练一个SVM对类别分类
训练一个线性回归模型预测边缘框的偏移
RoI（兴趣区域）池化层  给定一个锚框，均匀分成为n*m块，输出每块里的最大值
Fast R-CNN
使用CNN对图片抽取特征
使用RoI池化层对每个锚框生成固定长度的特征
Faster R-CNN
使用一个区域提议网络（Region proposal network）来替代启发式搜索获得更好的锚框
two-stage
精度相对较高，但是计算开销大
Mask R-CNN
如果有像素级别的标号，使用FCN（Fully convolutional network）来利用这些信息
在无人车领域应用较多
SSD 单发多框检测
one-stage
一个基础网络抽取特征，然后多个卷积层块来减半高宽
在每段都生成锚框 底部段拟合小物体，顶部段拟合大物体
对每个锚框预测类别和边缘框
YOLO
you only look once
将图片均匀地分为S*S个锚框
每个锚框预测B个边缘框
针对边缘框的形状等进行统计加入先验知识
工业界使用较多
CenterNet
对每个像素做预测  非锚框算法

## Q&A
基于锚框的算法对图片大小不敏感
高精度图片的小物体识别

## 多尺度目标检测代码实现
在特征图上生成锚框，每个单位（像素）作为锚框的中心
特征图较小时，可以选择较大尺度的锚框

## 双流网络论文精读
Andrej Karpathy  特斯拉AI总监
local minumum of 2D texture recognition
the data source has to become diverse videos
视频动作识别
motion-information
spatial stream ConvNet & Temporal stream ConvNet
optical flow is the pattern of apparent motion of objects, surfaces and edges in a visual scene caused by the relative motion between an observer and a scene.
光流可拆分为水平方向与竖直方向上的位移
每个像素点都有对应的光流值
L帧光流会得到L-1帧光流
抽取光流比较耗时，存储光流值比较占空间
如果模型难以学习某些信息，那就直接人工提供这些信息


## Day 13 ##
## SSD的代码实现
cls_predictor()
bbox_predictor()
5个stage
cls_loss = nn.CrossEntropyLoss(reduction='none')
bbox_loss = nn.L1Loss(reduction='none')
mask用于不计算背景框的loss

## Q&A
如果图像尺寸很大，不太适合用SSD，可以考虑R-CNN或yolo

## 语义分割和数据集
语义分割是将图片中的每个像素分类到对应的类别
应用：背景虚化  路面分割  实例分割（Dog1, Dog2, Cat1）
                      语义分割（Dog, Dog, Cat）
Pascal VOC2012数据集  VOC格式
训练数据是图片，标号也是图片，两者尺寸相同
RGB -> integer

## Q&A
关键点识别
通过采集更多数据来弥补数据质量的不足














