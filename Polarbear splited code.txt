import os, sys, argparse, random;\
import tensorflow as tf;\
from tensorflow.python.keras import backend as K;\
import numpy as np;\
import pandas as pd;\
from sklearn import preprocessing, metrics;\
from sklearn.linear_model import RidgeCV;\
import scipy;\
from scipy.io import mmread;\
sys.path.append('/fs/home/jiluzhang/Polarbear/bin/');\
from train_model import TranslateAE;\
from evaluation_functions import *

learning_rate_x = 0.001;\
learning_rate_y = 0.0001;\
learning_rate_xy = 0.001;\
learning_rate_yx = 0.001;\
embed_dim_x = 25;\
embed_dim_y = 25;\
dropout_rate = 0.1;\
nlayer = 2;\
batch_size = 16;\
trans_ver = 'linear';\
patience = 45;\
nepoch_warmup_x = 400;\
nepoch_warmup_y = 80;\
nepoch_klstart_x = 0;\
nepoch_klstart_y = 0;\
dispersion = 'genebatch';\
hidden_frac = 2;\
kl_weight = 1;\
train_test_split = 'babel'

##train_polarbear_model()
outdir='output_semi_gpu';\
sim_url = outdir + 'polarbear_'+ train_test_split +'_'+ dispersion + '_'+ str(nlayer)+ 'l_lr'+ str(learning_rate_y)+'_'+ str(learning_rate_x)+'_'+ str(learning_rate_xy)+'_'+ str(learning_rate_yx)+'_dropout'+ str(dropout_rate)+'_ndim'+str(embed_dim_x)+'_'+str(embed_dim_y)+'_batch'+ str(batch_size)+ '_'+ trans_ver + '_improvement'+str(patience)+'_nwarmup_'+str(nepoch_warmup_x)+'_'+str(nepoch_warmup_y)+'_klstart'+str(nepoch_klstart_x)+'_'+ str(nepoch_klstart_y)+'_klweight'+str(kl_weight)+'_hiddenfrac'+str(hidden_frac);\
path_x='adultbrainfull50_rna_outer_snareseq.mtx';\
path_y='adultbrainfull50_atac_outer_snareseq.mtx';\
path_x_single='adultbrainfull50_rna_outer_single.mtx';\
path_y_single='adultbrainfull50_atac_outer_single.mtx';\


os.system('mkdir -p '+ outdir);\
chr_annot = pd.read_csv(path_y.split('snareseq')[0]+ 'peaks.txt', sep=':', header=None);\
chr_annot.columns = ['chr','pos'];\

chr_list = {}
for chri in chr_annot['chr'].unique():
    if chri not in ['chrX','chrY']:
        chr_list[int(chri[3:])] = [i for i, x in enumerate(chr_annot['chr']) if x == chri];

chr_list_range = []
for chri in chr_list.keys():
    chr_list_range += chr_list[chri]

chr_annot.iloc[chr_list_range].to_csv(sim_url+'_peaks.txt', index=False, sep=':', header=None);\

def load_rna_file_sparse(url1):
    data_rna = mmread(url1).tocsr();\
    data_rna = data_rna.astype(int);\
    data_rna_batch = mmread(url1.split('.mtx')[0]+'_barcodes_dataset.mtx').tocsr();\
    return data_rna, data_rna_batch

def load_atac_file_sparse(url2, index_range):
    data_atac = mmread(url2).tocsr()[:,index_range];\
    data_atac[data_atac != 0] = 1;\
    data_atac = data_atac.astype(int);\
    data_atac_batch = mmread(url2.split('.mtx')[0]+'_barcodes_dataset.mtx').tocsr();\
    return data_atac, data_atac_batch


data_rna, data_rna_batch = load_rna_file_sparse(path_x);\
data_atac, data_atac_batch = load_atac_file_sparse(path_y, chr_list_range);\

data_rna_barcode = pd.read_csv(path_x.split('.mtx')[0]+ '_barcodes.tsv', delimiter='\t');\
barcode_list = data_rna_barcode['index'].to_list();\


with open('./data/babel_test_barcodes.txt') as fp:
    test_barcode = fp.read().splitlines()

with open('./data/babel_valid_barcodes.txt') as fp:
    valid_barcode = fp.read().splitlines()

with open('./data/babel_train_barcodes.txt') as fp:
    train_barcode = fp.read().splitlines()


train_index = [barcode_list.index(x) for x in train_barcode];\
val_index = [barcode_list.index(x) for x in valid_barcode];\
test_index = [barcode_list.index(x) for x in test_barcode];\
data_rna_train = data_rna[train_index,];\
batch_rna_train = data_rna_batch[train_index,];\
data_rna_train_co = data_rna[train_index,];\
batch_rna_train_co = data_rna_batch[train_index,];\
data_rna_test = data_rna[test_index,];\
batch_rna_test = data_rna_batch[test_index,];\
data_rna_val = data_rna[val_index,];\
batch_rna_val = data_rna_batch[val_index,];\
data_atac_train = data_atac[train_index,];\
batch_atac_train = data_atac_batch[train_index,];\
data_atac_train_co = data_atac[train_index,];\
batch_atac_train_co = data_atac_batch[train_index,];\
data_atac_test = data_atac[test_index,];\
batch_atac_test = data_atac_batch[test_index,];\
data_atac_val = data_atac[val_index,];\
batch_atac_val = data_atac_batch[val_index,];\



train_barcode = np.array(barcode_list)[train_index];\
valid_barcode = np.array(barcode_list)[val_index];\
test_barcode = np.array(barcode_list)[test_index];\
np.savetxt(sim_url+'_train_barcodes.txt', train_barcode, delimiter='\n', fmt='%s');\
np.savetxt(sim_url+'_valid_barcodes.txt', valid_barcode, delimiter='\n', fmt='%s');\
np.savetxt(sim_url+'_test_barcodes.txt', test_barcode, delimiter='\n', fmt='%s');\


data_rna_single, data_rna_single_batch = load_rna_file_sparse(path_x_single);\   # ~4 min
data_rna_train = scipy.sparse.vstack((data_rna_train, data_rna_single));\
batch_rna_train = scipy.sparse.vstack((batch_rna_train, data_rna_single_batch));\

data_atac_single, data_atac_single_batch = load_atac_file_sparse(path_y_single, chr_list_range);\  # 5 min
data_atac_train = scipy.sparse.vstack((data_atac_train, data_atac_single));\
batch_atac_train = scipy.sparse.vstack((batch_atac_train, data_atac_single_batch));\

rand_index_rna = list(range(data_rna_train.shape[0]));\
random.seed(101);\
random.shuffle(rand_index_rna);\
data_rna_train = data_rna_train[rand_index_rna,];\
batch_rna_train = batch_rna_train[rand_index_rna,]

rand_index_atac = list(range(data_atac_train.shape[0]));\
random.seed(101);\
random.shuffle(rand_index_atac);\
data_atac_train = data_atac_train[rand_index_atac,];\
batch_atac_train = batch_atac_train[rand_index_atac,]

os.environ["CUDA_VISIBLE_DEVICES"] = "5"

head -n 10000003 adultbrainfull50_atac_outer_single_raw.mtx > adultbrainfull50_atac_outer_single.mtx
head -n 10000003 adultbrainfull50_rna_outer_single_raw.mtx > adultbrainfull50_rna_outer_single.mtx

import importlib
importlib.reload(train_model)

tf.reset_default_graph();\

autoencoder = train_model.TranslateAE(input_dim_x=data_rna_val.shape[1], input_dim_y=data_atac_val.shape[1], batch_dim_x=batch_rna_val.shape[1], batch_dim_y=batch_atac_val.shape[1], \
                          embed_dim_x=embed_dim_x, embed_dim_y=embed_dim_y, dispersion=dispersion, chr_list=chr_list, nlayer=nlayer, dropout_rate=dropout_rate, \
                          output_model=sim_url, learning_rate_x=learning_rate_x, learning_rate_y=learning_rate_y, learning_rate_xy=learning_rate_xy, learning_rate_yx=learning_rate_yx, \
                          trans_ver=trans_ver, hidden_frac=hidden_frac, kl_weight=kl_weight);\

iter_list1, iter_list2, iter_list3, iter_list4, val_reconstr_atac_loss_list, val_kl_atac_loss_list, val_reconstr_rna_loss_list, val_kl_rna_loss_list, val_translat_atac_loss_list, val_translat_rna_loss_list = autoencoder.train(data_rna_train, batch_rna_train, data_atac_train, batch_atac_train, data_rna_val, batch_rna_val, data_atac_val, batch_atac_val, \
                                               data_rna_train_co, batch_rna_train_co, data_atac_train_co, batch_atac_train_co, \
                                               nepoch_warmup_x, nepoch_warmup_y, patience, nepoch_klstart_x, nepoch_klstart_y, \
                                               output_model=sim_url, batch_size=32, nlayer=nlayer, save_model=True);\

data_x, batch_x, data_y, batch_y, data_x_val, batch_x_val, data_y_val, batch_y_val, \
data_x_co, batch_x_co, data_y_co, batch_y_co, \
nepoch_warmup_x, nepoch_warmup_y, patience, nepoch_klstart_x, nepoch_klstart_y, \
output_model,  batch_size, nlayer, save_model = \
data_rna_train, batch_rna_train, data_atac_train, batch_atac_train, data_rna_val, batch_rna_val, data_atac_val, batch_atac_val, \
data_rna_train_co, batch_rna_train_co, data_atac_train_co, batch_atac_train_co, \
nepoch_warmup_x, nepoch_warmup_y, patience, nepoch_klstart_x, nepoch_klstart_y, \
sim_url, batch_size, nlayer, True

val_reconstr_y_loss_list = [];\
val_kl_y_loss_list = [];\
val_reconstr_x_loss_list = [];\
val_kl_x_loss_list = [];\
val_translat_y_loss_list = [];\
val_translat_x_loss_list = [];\
last_improvement=0;\
n_iter_step1 = 0;\
n_iter_step2 = 0

iter_list1 = [];\
iter_list2 = [];\
iter_list3 = [];\
iter_list4 = [];\
loss_val_check_list = [];\
my_epochs_max = {};\
my_epochs_max[1] = 500;\
my_epochs_max[2] = 500;\
my_epochs_max[3] = 100;\
my_epochs_max[4] = 100;\
saver = tf.train.Saver();\
sep_train_index = 1



from datetime import datetime
print(datetime.now())




















2023-09-08 14:19:43.309368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: NVIDIA A800 80GB PCIe major: 8 minor: 0 memoryClockRate(GHz): 1.41
pciBusID: 0000:9d:00.0
2023-09-08 14:19:43.309466: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2023-09-08 14:19:43.309513: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2023-09-08 14:19:43.309554: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2023-09-08 14:19:43.309597: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2023-09-08 14:19:43.309639: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2023-09-08 14:19:43.309679: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2023-09-08 14:19:43.309723: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:
2023-09-08 14:19:43.309731: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1662] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-09-08 14:19:43.309902: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2023-09-08 14:19:43.317635: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2900000000 Hz
2023-09-08 14:19:43.319828: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5604942170f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2023-09-08 14:19:43.319847: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2023-09-08 14:19:43.454775: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560496281990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-09-08 14:19:43.454830: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A800 80GB PCIe, Compute Capability 8.0
2023-09-08 14:19:43.455037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-09-08 14:19:43.455057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      
WARNING:tensorflow:From /fs/home/jiluzhang/Polarbear/bin/train_model.py:338: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.









